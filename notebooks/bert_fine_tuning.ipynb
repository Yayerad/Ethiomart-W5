{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "\n",
    "import requests\n",
    "\n",
    "# Step 1: Download the file from Google Drive\n",
    "file_id = \"1qpzy8eeqlSzkSN4g4yVLAa_ce0ZUovUh\"  # Replace with your file ID\n",
    "download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "file_path = \"dataset.conll\"  # Path to save the file locally\n",
    "\n",
    "response = requests.get(download_url)\n",
    "with open(file_path, \"wb\") as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "print(f\"File downloaded and saved as {file_path}\")\n",
    "\n",
    "# Step 2: Load and process the data from .conll file\n",
    "def load_conll_data(file_path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    sentence = []\n",
    "    label = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()  # Remove leading/trailing whitespaces\n",
    "            if line:  # Non-empty line\n",
    "                token, entity = line.split()  # Split token and label\n",
    "                sentence.append(token)\n",
    "                label.append(entity)\n",
    "            else:  # Empty line (end of a sentence)\n",
    "                if sentence:  # Only add non-empty sentences\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                sentence = []  # Reset sentence and label for next sentence\n",
    "                label = []  # Reset for next sentence\n",
    "\n",
    "        # Add the last sentence (if the file doesn't end with an empty line)\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "# Load dataset from the downloaded .conll file\n",
    "sentences, labels = load_conll_data(file_path)\n",
    "\n",
    "# Step 3: Convert to Hugging Face Dataset format\n",
    "data = {\"tokens\": sentences, \"ner_tags\": labels}\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict(data)\n",
    "})\n",
    "\n",
    "# Check if data is loaded correctly\n",
    "print(f\"Loaded {len(sentences)} sentences with {len(labels)} labels.\")\n",
    "\n",
    "# Step 4: Load the BERT Tiny Amharic model and tokenizer\n",
    "model_name = \"olivertab/bert-tiny-amharic-uncased\"  # Replace with the actual model name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=5)  # Adjust `num_labels` to your data\n",
    "\n",
    "# Step 5: Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"tokens\"], truncation=True, padding=\"max_length\", is_split_into_words=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Step 6: Set up data collator for token classification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Step 7: Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Step 8: Train the model using Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Optionally: Save the model\n",
    "trainer.save_model(\"bert_tiny_amharic_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
